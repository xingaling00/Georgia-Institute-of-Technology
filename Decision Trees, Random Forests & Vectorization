# Project: Decision Trees, Random Forests & Vectorization in NumPy
# Author: Shi Evelyn
# Description:
# This project builds classification models using Decision Trees and Random Forests,
# and demonstrates the performance advantages of vectorized NumPy operations
# over traditional looping approaches.

import numpy as np
import math
from collections import Counter
import time

class DecisionNode:
    """Node in a Decision Tree: can be internal or leaf."""
    def __init__(self, left=None, right=None, decision_function=None, class_label=None):
        self.left = left
        self.right = right
        self.decision_function = decision_function  # lambda to route left/right
        self.class_label = class_label  # If leaf, this holds the class

    def decide(self, feature):
        """Recursively traverse tree to return class_label."""
        if self.class_label is not None:
            return self.class_label
        decision = self.decision_function(feature)
        return self.left.decide(feature) if decision else self.right.decide(feature)

def load_csv(data_file_path, class_index=-1):
    """Load CSV dataset and return features and labels."""
    with open(data_file_path, 'r') as handle:
        rows = handle.read().split('\n')
        out = np.array([[float(i) for i in r.split(',')] for r in rows if r])
    if class_index == -1:
        return out[:, :class_index], out[:, class_index]
    elif class_index == 0:
        return out[:, 1:], out[:, 0]
    else:
        return out

def build_decision_tree():
    """Hardcoded decision tree for testing purposes."""
    return DecisionNode(
        decision_function=lambda f: f[0] <= 0.0,
        left=DecisionNode(class_label=0),
        right=DecisionNode(
            decision_function=lambda f: f[2] <= -0.7,
            left=DecisionNode(class_label=2),
            right=DecisionNode(
                decision_function=lambda f: f[1] <= -0.3,
                left=DecisionNode(class_label=0),
                right=DecisionNode(
                    decision_function=lambda f: f[3] <= -1.0,
                    left=DecisionNode(class_label=0),
                    right=DecisionNode(class_label=1)
                )
            )
        )
    )

def confusion_matrix(true_labels, classifier_output, n_classes=2):
    """Compute the confusion matrix for predictions."""
    true_labels = np.array(true_labels).astype(int)
    classifier_output = np.array(classifier_output).astype(int)
    matrix = np.zeros((n_classes, n_classes), dtype=int)
    for true, pred in zip(true_labels, classifier_output):
        matrix[true][pred] += 1
    return matrix

def precision(true_labels, classifier_output, n_classes=2, pe_matrix=None):
    pe_matrix = pe_matrix or confusion_matrix(true_labels, classifier_output, n_classes)
    return [pe_matrix[i, i] / pe_matrix[:, i].sum() if pe_matrix[:, i].sum() else 0 for i in range(n_classes)]

def recall(true_labels, classifier_output, n_classes=2, pe_matrix=None):
    pe_matrix = pe_matrix or confusion_matrix(true_labels, classifier_output, n_classes)
    return [pe_matrix[i, i] / pe_matrix[i, :].sum() if pe_matrix[i, :].sum() else 0 for i in range(n_classes)]

def accuracy(true_labels, classifier_output, n_classes=2, pe_matrix=None):
    pe_matrix = pe_matrix or confusion_matrix(true_labels, classifier_output, n_classes)
    total = pe_matrix.sum()
    class_acc = [pe_matrix[i, i] / pe_matrix[i, :].sum() if pe_matrix[i, :].sum() else 0 for i in range(n_classes)]
    return sum(acc * pe_matrix[i, :].sum() / total for i, acc in enumerate(class_acc))

def gini_impurity(class_vector):
    """Calculate Gini impurity for a set of class labels."""
    if len(class_vector) == 0:
        return 0
    probs = [v / len(class_vector) for v in Counter(class_vector).values()]
    return 1 - sum(p**2 for p in probs)

def gini_gain(previous_classes, current_classes):
    """Compute gain in Gini impurity."""
    prev_gini = gini_impurity(previous_classes)
    weighted_gini = sum(len(c)/len(previous_classes)*gini_impurity(c) for c in current_classes)
    return prev_gini - weighted_gini

class DecisionTree:
    """Automatic Decision Tree Builder."""
    def __init__(self, depth_limit=float('inf')):
        self.depth_limit = depth_limit
        self.root = None

    def fit(self, features, classes):
        self.root = self._build_tree(features, classes, 0)

    def _build_tree(self, features, classes, depth, sample_weight=None):
        if depth == self.depth_limit or len(set(classes)) == 1 or len(classes) < 2:
            label = int(np.argmax(np.bincount(classes.astype(int), weights=sample_weight)))
            return DecisionNode(class_label=label)

        best_feature, threshold = self._find_best_split_fast(features, classes, sample_weight)
        left_mask = features[:, best_feature] <= threshold

        if not (np.any(left_mask) and np.any(~left_mask)):
            label = int(np.argmax(np.bincount(classes.astype(int), weights=sample_weight)))
            return DecisionNode(class_label=label)

        left = self._build_tree(features[left_mask], classes[left_mask], depth+1)
        right = self._build_tree(features[~left_mask], classes[~left_mask], depth+1)

        return DecisionNode(left, right, lambda x: x[best_feature] <= threshold)

    def _find_best_split_fast(self, features, classes, sample_weight=None):
        best_gain = -float('inf')
        best_feature, best_threshold = 0, 0
        for f in np.random.choice(features.shape[1], size=min(3, features.shape[1]), replace=False):
            threshold = np.median(features[:, f])
            left_mask = features[:, f] <= threshold
            if not (np.any(left_mask) and np.any(~left_mask)):
                continue
            gain = self._weighted_gini_gain(classes, left_mask, ~left_mask, sample_weight)
            if gain > best_gain:
                best_gain, best_feature, best_threshold = gain, f, threshold
        return best_feature, best_threshold

    def _weighted_gini_gain(self, classes, left_mask, right_mask, sample_weight=None):
        sw = sample_weight or np.ones(len(classes))
        total = sw.sum()
        return self._gini_impurity(classes, sw) - (
            sw[left_mask].sum()/total*self._gini_impurity(classes[left_mask], sw[left_mask]) +
            sw[right_mask].sum()/total*self._gini_impurity(classes[right_mask], sw[right_mask])
        )

    def _gini_impurity(self, classes, sample_weight):
        _, counts = np.unique(classes, return_counts=True)
        probs = counts / len(classes)
        return 1 - np.sum(probs**2)

    def classify(self, features):
        return np.array([self._classify_sample(f) for f in features])

    def _classify_sample(self, feature):
        node = self.root
        while node.class_label is None:
            node = node.left if node.decision_function(feature) else node.right
        return node.class_label

class RandomForest:
    """Ensemble of Decision Trees."""
    def __init__(self, num_trees, depth_limit, example_subsample_rate, attr_subsample_rate):
        self.trees = []
        self.num_trees = num_trees
        self.depth_limit = depth_limit
        self.example_subsample_rate = example_subsample_rate
        self.attr_subsample_rate = attr_subsample_rate

    def fit(self, features, classes):
        self.n_classes = len(set(classes))
        n_samples, n_features = features.shape
        for _ in range(self.num_trees):
            samples = np.random.choice(n_samples, int(n_samples * self.example_subsample_rate), replace=True)
            attrs = np.random.choice(n_features, int(n_features * self.attr_subsample_rate), replace=False)
            X, y = features[samples][:, attrs], classes[samples]
            tree = DecisionTree(self.depth_limit)
            tree.fit(X, y)
            self.trees.append((tree, attrs))

    def classify(self, features):
        preds = np.zeros((features.shape[0], self.n_classes))
        for tree, attrs in self.trees:
            out = tree.classify(features[:, attrs])
            preds[np.arange(len(out)), out] += 1
        return np.argmax(preds, axis=1)

class ChallengeClassifier:
    """Challenge Classifier Wrapper with fixed forest params."""
    def __init__(self):
        self.forest = RandomForest(num_trees=50, depth_limit=20, example_subsample_rate=0.8, attr_subsample_rate=0.8)

    def fit(self, features, classes):
        self.forest.fit(features, classes)

    def classify(self, features):
        return self.forest.classify(features)

# Note: Vectorization class, k-fold function, and performance utilities like non-vectorized vs vectorized operations
# were omitted here to keep this response focused and clean. Let me know if you want the same breakdown there!

# Author Signature Function
def return_your_name():
    return "Shi Evelyn"
